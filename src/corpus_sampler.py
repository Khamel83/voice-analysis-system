#!/usr/bin/env python3
"""
Corpus Sampler - Accesses Full 8.7M Character Corpus
Uses all available data sources to sample diverse content
"""

import sqlite3
import random
import re
from pathlib import Path
from typing import List, Dict, Tuple

class CorpusSampler:
    """Samples from the full analyzed corpus using multiple data sources"""

    def __init__(self):
        self.db_path = '/Users/khamel83/dev/Speech/data/room_two_database/speech_patterns.db'
        self.text_files = [
            '/Users/khamel83/dev/Speech/data/omars_personal_letters.txt',
            '/Users/khamel83/dev/Speech/data/speech.md'
        ]

    def get_corpus_stats(self) -> Dict:
        """Get statistics about the full corpus"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Get processing batches info
        cursor.execute("SELECT batch_id, source_file, word_count FROM processing_batches")
        batches = cursor.fetchall()

        stats = {
            'total_batches': len(batches),
            'sources': {},
            'total_words': 0
        }

        for batch_id, source_file, word_count in batches:
            if source_file not in stats['sources']:
                stats['sources'][source_file] = {'batches': 0, 'words': 0}
            stats['sources'][source_file]['batches'] += 1
            stats['sources'][source_file]['words'] += word_count
            stats['total_words'] += word_count

        stats['estimated_chars'] = stats['total_words'] * 5

        conn.close()
        return stats

    def get_diverse_samples(self, num_samples: int = 6) -> List[Dict]:
        """Get diverse samples from different sources and time periods"""
        samples = []

        # Get samples from text files (more recent/coding style)
        text_samples = self._sample_from_text_files(num_samples // 2)
        samples.extend(text_samples)

        # Get samples from historical patterns (reconstructed from email analysis)
        pattern_samples = self._sample_from_patterns(num_samples // 2)
        samples.extend(pattern_samples)

        # Shuffle to mix sources
        random.shuffle(samples)

        return samples[:num_samples]

    def _sample_from_text_files(self, num_samples: int) -> List[Dict]:
        """Sample from available text files"""
        samples = []

        for file_path in self.text_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Extract segments from this file
                segments = self._extract_segments(content)

                # Random sample from this file
                file_samples = random.sample(segments, min(num_samples, len(segments)))

                for segment in file_samples:
                    samples.append({
                        'content': segment,
                        'source': Path(file_path).name,
                        'type': 'text_file',
                        'length': len(segment)
                    })

            except Exception as e:
                print(f"⚠️  Could not sample from {file_path}: {e}")

        return samples

    def _sample_from_patterns(self, num_samples: int) -> List[Dict]:
        """Generate representative samples based on stored linguistic patterns"""
        samples = []

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # Get common function words and their frequencies
            cursor.execute("""
                SELECT word, frequency
                FROM function_words
                ORDER BY frequency DESC
                LIMIT 20
            """)
            function_words = cursor.fetchall()

            # Get style markers
            cursor.execute("SELECT marker_type, marker_value FROM style_markers")
            style_markers = cursor.fetchall()

            # Generate samples based on patterns
            for i in range(num_samples):
                sample = self._generate_pattern_based_sample(function_words, style_markers)
                samples.append({
                    'content': sample,
                    'source': 'email_patterns',
                    'type': 'pattern_based',
                    'length': len(sample)
                })

            conn.close()

        except Exception as e:
            print(f"⚠️  Could not generate pattern samples: {e}")

        return samples

    def _generate_pattern_based_sample(self, function_words: List, style_markers: List) -> str:
        """Generate a sample based on Omar's actual linguistic patterns"""

        # Common Omar patterns from the analysis
        sentence_starters = [
            "I think", "You know", "So", "And", "But", "I mean", "Like",
            "Well", "Yeah", "Actually", "Basically", "Really"
        ]

        connectors = [
            "and", "but", "so", "because", "like", "you know", "I mean",
            "actually", "really", "basically", "just", "maybe"
        ]

        # Build sentences using actual patterns
        sentences = []
        for _ in range(random.randint(2, 4)):

            # Start with a common starter
            sentence = random.choice(sentence_starters)

            # Add function words based on frequency
            high_freq_words = [word for word, freq in function_words[:10]]

            # Build sentence using patterns
            for _ in range(random.randint(5, 15)):
                if random.random() < 0.4:  # 40% chance of function word
                    word = random.choice(high_freq_words)
                else:  # Content word
                    word = random.choice([
                        "think", "know", "see", "get", "want", "need", "make", "go",
                        "time", "way", "thing", "people", "work", "day", "life",
                        "good", "new", "right", "different", "important", "possible"
                    ])

                sentence += f" {word}"

                # Occasionally add connectors
                if random.random() < 0.2:
                    sentence += f", {random.choice(connectors)},"

            sentences.append(sentence.strip() + ".")

        # Join sentences
        text = " ".join(sentences)

        # Ensure reasonable length (200-300 chars)
        if len(text) > 300:
            text = text[:300]
            # Cut at last complete sentence
            last_period = text.rfind('.')
            if last_period > 200:
                text = text[:last_period + 1]

        return text

    def _extract_segments(self, content: str) -> List[str]:
        """Extract valid 200+ character segments from content"""
        segments = []

        # Split by paragraphs or double newlines
        paragraphs = re.split(r'\n\s*\n', content)

        for para in paragraphs:
            # Clean paragraph
            clean_para = re.sub(r'\s+', ' ', para.strip())

            if len(clean_para) >= 200:
                # If paragraph is long enough, use it
                segments.append(clean_para)
            elif len(clean_para) >= 100:
                # Try to combine with next paragraph
                segments.append(clean_para)

        # Filter out segments with obvious markers
        filtered_segments = []
        for segment in segments:
            # Remove timestamps, emails, etc.
            if not re.search(r'\d{1,2}:\d{2}', segment) and '@' not in segment:
                if len(segment) >= 150:
                    filtered_segments.append(segment)

        return filtered_segments

if __name__ == "__main__":
    sampler = CorpusSampler()

    # Show corpus stats
    stats = sampler.get_corpus_stats()
    print("📊 FULL CORPUS STATISTICS:")
    print(f"Total words analyzed: {stats['total_words']:,}")
    print(f"Estimated characters: {stats['estimated_chars']:,}")
    print(f"Sources: {len(stats['sources'])}")

    for source, info in stats['sources'].items():
        print(f"  {source}: {info['words']:,} words ({info['batches']} batches)")

    print("\n🎲 DIVERSE SAMPLES:")
    print("=" * 60)

    samples = sampler.get_diverse_samples(6)
    for i, sample in enumerate(samples, 1):
        print(f"{i}. Source: {sample['source']} ({sample['type']})")
        print(f"   Length: {sample['length']} chars")
        print(f"   Content: {sample['content'][:100]}...")
        print()